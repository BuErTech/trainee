{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optical Character Recognition (OCR) is the conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a photo from a scene (billboards in a landscape photo) or from a text superimposed on an image (subtitles on a television broadcast).\n",
    "\n",
    "OCR consists generally of sub-processes to perform as accurately as possible.\n",
    "\n",
    "1.  Pre-processing\n",
    "2.  Text detection\n",
    "3.  Text recognition\n",
    "4.  Post-processing\n",
    "\n",
    "The sub-processes can of course vary depending on the use-case but these are generaly the steps needed to perform optical character recognition.\n",
    "\n",
    "## Tesseract OCR :\n",
    "\n",
    "Tesseract is an open source text recognition (OCR) Engine, available under the Apache 2.0 license. It can be used directly, or (for programmers) using an API to extract printed text from images. It supports a wide variety of languages. Tesseract doesn’t have a built-in GUI, but there are several available from the [3rdParty page](https://github.com/tesseract-ocr/tesseract/wiki/User-Projects-%E2%80%93-3rdParty). Tesseract is compatible with many programming languages and frameworks through wrappers that can be found [here](https://github.com/tesseract-ocr/tesseract/wiki/AddOns). It can be used with the existing layout analysis to recognize text within a large document, or it can be used in conjunction with an external text detector to recognize text from an image of a single text line.\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitf1i4vb9j30jg0am0t7.jpg)\n",
    "\n",
    "Tesseract 4.00 includes a new neural network subsystem configured as a text line recognizer. It has its origins in [OCRopus’ Python-based LSTM](https://github.com/tmbdev/ocropy) implementation but has been redesigned for Tesseract in C++. The neural network system in Tesseract pre-dates TensorFlow but is compatible with it, as there is a network description language called Variable Graph Specification Language (VGSL), that is also available for TensorFlow.\n",
    "\n",
    "To recognize an image containing a single character, we typically use a Convolutional Neural Network (CNN). Text of arbitrary length is a sequence of characters, and such problems are solved using RNNs and LSTM is a popular form of RNN. Read this post to learn more about [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "# How it works\n",
    "\n",
    "Tesseract developed from [OCRopus](https://github.com/tmbarchive/ocropy) model in Python which was a fork of a LSMT in C++, called CLSTM. CLSTM is an implementation of the LSTM recurrent neural network model in C++.\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitf281kv5j30jg089q3r.jpg)\n",
    "\n",
    "Tesseract was an effort on code cleaning and adding a new LSTM model. The input image is processed in boxes (rectangle) line by line feeding into the LSTM model and giving output. In the image below we can visualize how it works.\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitf2qoaavj30jg0aw75o.jpg)\n",
    "\n",
    "# Installing Tesseract\n",
    "\n",
    "Installing tesseract on Windows is easy with the precompiled binaries found [here](https://digi.bib.uni-mannheim.de/tesseract/). Do not forget to edit “path” environment variable and add tesseract path. For Linux or Mac installation it is installed with [few commands](https://github.com/tesseract-ocr/tesseract/wiki).\n",
    "\n",
    "By default, Tesseract expects a page of text when it segments an image. If you’re just seeking to OCR a small region, try a different segmentation mode, using the _— psm_ argument. There are 14 modes available which can be found [here](https://tesseract-ocr.github.io/tessdoc/ImproveQuality#page-segmentation-method). By default, Tesseract fully automates the page segmentation but does not perform orientation and script detection. To specify the parameter, type the following:\n",
    "\n",
    "  0    Orientation and script detection (OSD) only.  \n",
    "  1    Automatic page segmentation with OSD.  \n",
    "  2    Automatic page segmentation, but no OSD, or OCR.  \n",
    "  3    Fully automatic page segmentation, but no OSD. (Default)  \n",
    "  4    Assume a single column of text of variable sizes.  \n",
    "  5    Assume a single uniform block of vertically aligned text.  \n",
    "  6    Assume a single uniform block of text.  \n",
    "  7    Treat the image as a single text line.  \n",
    "  8    Treat the image as a single word.  \n",
    "  9    Treat the image as a single word in a circle.  \n",
    " 10    Treat the image as a single character.  \n",
    " 11    Sparse text. Find as much text as possible in no particular order.  \n",
    " 12    Sparse text with OSD.  \n",
    " 13    Raw line. Treat the image as a single text line,  bypassing hacks that are Tesseract-specific.\n",
    " \n",
    " There is also one more important argument, OCR engine mode (oem). Tesseract 4 has two OCR engines — Legacy Tesseract engine and LSTM engine. There are four modes of operation chosen using the — oem option.\n",
    "\n",
    "0. Legacy engine only.  \n",
    "1. Neural nets LSTM engine only.  \n",
    "2. **Legacy** + LSTM engines.  \n",
    "3. Default, based on what is available.\n",
    "\n",
    "# OCR with Pytesseract and OpenCV :\n",
    "\n",
    "**Pytesseract** is a wrapper for Tesseract-OCR Engine. It is also useful as a stand-alone invocation script to tesseract, as it can read all image types supported by the Pillow and Leptonica imaging libraries, including jpeg, png, gif, bmp, tiff, and others. More info about Python approach read [here](https://github.com/madmaze/pytesseract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import pytesseract\n",
    "\n",
    "img = cv2.imread('image.jpg')\n",
    "\n",
    "# Adding custom options\n",
    "custom_config = r'--oem 3 --psm 6'\n",
    "pytesseract.image_to_string(img, config=custom_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Tesseract :\n",
    "\n",
    "We need to make sure the image is appropriately [pre-processed](https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality#image-processing). to ensure a certain level of accuracy.\n",
    "\n",
    "This includes rescaling, binarization, noise removal, deskewing, etc.\n",
    "\n",
    "To preprocess image for OCR, use any of the following python functions or follow the [OpenCV documentation](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_table_of_contents_imgproc/py_table_of_contents_imgproc.html).\n",
    "\n",
    "<figure class=\"hq hr hs ht hu hv\">\n",
    "\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('image.jpg')\n",
    "\n",
    "# get grayscale image\n",
    "def get_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "# noise removal\n",
    "def remove_noise(image):\n",
    "    return cv2.medianBlur(image,5)\n",
    " \n",
    "#thresholding\n",
    "def thresholding(image):\n",
    "    # threshold the image, setting all foreground pixels to\n",
    "    # 255 and all background pixels to 0\n",
    "    return cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "#dilation\n",
    "def dilate(image):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    return cv2.dilate(image, kernel, iterations = 1)\n",
    "    \n",
    "#erosion\n",
    "def erode(image):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    return cv2.erode(image, kernel, iterations = 1)\n",
    "\n",
    "#opening - erosion followed by dilation\n",
    "def opening(image):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "#canny edge detection\n",
    "def canny(image):\n",
    "    return cv2.Canny(image, 100, 200)\n",
    "\n",
    "#skew correction\n",
    "def deskew(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bitwise_not(gray)\n",
    "    thresh = cv2.threshold(gray, 0, 255,\n",
    "        cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    coords = np.column_stack(np.where(thresh > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h),\n",
    "        flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)    \n",
    "    return rotated\n",
    "\n",
    "#template matching\n",
    "def match_template(image, template):\n",
    "    return cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deskew = deskew(image)\n",
    "gray = get_grayscale(deskew)\n",
    "thresh = thresholding(gray)\n",
    "rnoise = remove_noise(gray)\n",
    "dilate = dilate(gray)\n",
    "erode = erode(gray)\n",
    "opening = opening(gray)\n",
    "canny = canny(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_images(images, cols = 1, titles = None):\n",
    "   \n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()\n",
    "    \n",
    "show_images(images, 3, [\"gray\",\"rnoise\",\"dilate\",\"erode\",\"thresh\",\"deskew\",\"opening\",\"canny\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input is this image :\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitfb1hwhtj308306dt93.jpg)\n",
    "\n",
    "Here’s what we get :\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitfbc3sdbj30jg0cm75x.jpg)\n",
    "\n",
    "# Getting boxes around text :\n",
    "\n",
    "We can determine the bounding box information with PyTesseradt using the following [code](https://stackoverflow.com/questions/20831612/getting-the-bounding-box-of-the-recognized-words-using-python-tesseract).\n",
    "\n",
    "The script below will give you bounding box information for each character detected by tesseract during OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "img = cv2.imread('writings.jpg')\n",
    "\n",
    "h, w, c = img.shape\n",
    "boxes = pytesseract.image_to_boxes(img) \n",
    "for b in boxes.splitlines():\n",
    "    b = b.split(' ')\n",
    "    img = cv2.rectangle(img, (int(b[1]), h - int(b[2])), (int(b[3]), h - int(b[4])), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want boxes around words instead of characters, the function `image_to_data` will come in handy. You can use the `image_to_data` function with output type specified with pytesseract `Output`.\n",
    "\n",
    "We will use the sample receipt image below as input to test out tesseract .\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitflb7yp9j30bw0i0gni.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "img = cv2.imread('recu.jpg')\n",
    "\n",
    "d = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
    "print(d.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a dictionary with the followings keys :\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitfq48nv7j30eu00t0sq.jpg)\n",
    "\n",
    "Using this dictionary, we can get each word detected, their bounding box information, the text in them and the confidence scores for each.\n",
    "\n",
    "You can plot the boxes by using the code below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boxes = len(d['text'])\n",
    "for i in range(n_boxes):\n",
    "    if int(d['conf'][i]) > 60:\n",
    "        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
    "        img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output:\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitfs9dx0tj30bt0hz416.jpg)\n",
    "\n",
    "As we can see Tesseract is not capable to detect all text boxes confidently, poor quality scans and small fonts may produce poor quality OCR text detection. Also no preprocessing have been done to improve the quality of the image.\n",
    "\n",
    "# Text template matching ( detect only digits ):\n",
    "\n",
    "Take the example of trying to find where a only digits string is in an image. Here our template will be a regular expression pattern that we will match with our OCR results to find the appropriate bounding boxes. We will use the `regex` module and the `image_to_data` function for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "img = cv2.imread(r\"recu.jpg\")\n",
    "d = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
    "keys = list(d.keys())\n",
    "\n",
    "date_pattern = '^[0-9]*$'\n",
    "\n",
    "n_boxes = len(d['text'])\n",
    "for i in range(n_boxes):\n",
    "    if int(d['conf'][i]) > 60:\n",
    "        if re.match(date_pattern, d['text'][i]):\n",
    "            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
    "            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitfx9lzh7j30bw0hq76l.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page segmentation modes :\n",
    "\n",
    "There are several ways a page of text can be analysed. The tesseract api provides several page segmentation modes if you want to run OCR on only a small region or in different orientations, etc.\n",
    "\n",
    "Here’s a list of the supported page segmentation modes by tesseract :\n",
    "\n",
    "0. Orientation and script detection (OSD) only.  \n",
    "1. Automatic page segmentation with OSD.  \n",
    "2. Automatic page segmentation, but no OSD, or OCR.  \n",
    "3. Fully automatic page segmentation, but no OSD. (Default)  \n",
    "4. Assume a single column of text of variable sizes.  \n",
    "5. Assume a single uniform block of vertically aligned text.  \n",
    "6. Assume a single uniform block of text.  \n",
    "7. Treat the image as a single text line.  \n",
    "8. Treat the image as a single word.  \n",
    "9. Treat the image as a single word in a circle.  \n",
    "10. Treat the image as a single character.  \n",
    "11. Sparse text. Find as much text as possible in no particular order.  \n",
    "12. Sparse text with OSD.  \n",
    "13. Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.\n",
    "\n",
    "To change your page segmentation mode, change the `--psm` argument in your custom config string to any of the above mentioned mode codes.\n",
    "\n",
    "## Detect only digits using configuration :\n",
    "\n",
    "You can recognise only digits by changing the config to the following :\n",
    "\n",
    "<figure class=\"hq hr hs ht hu hv\">\n",
    "\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = r'--oem 3 --psm 6 outputbase digits'\n",
    "print(pytesseract.image_to_string(img, config=custom_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives the following output.\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitfyo0ip2j305t05k3yn.jpg)\n",
    "\n",
    "As you can see the output is not the same using regex .\n",
    "\n",
    "# Whitelisting/Blacklisting characters :\n",
    "\n",
    "## Whitelisting letters :\n",
    "\n",
    "Say you only want to detect certain characters from the given image and ignore the rest. You can specify your whitelist of characters (here, we have used all the lowercase characters from a to z only) by using the following config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = r'-c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyz --psm 6'\n",
    "print(pytesseract.image_to_string(img, config=custom_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it gives us this output :\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitfzlusrij305t06saac.jpg)\n",
    "\n",
    "## Blacklisting letters :\n",
    "\n",
    "If you are sure some characters or expressions definitely will not turn up in your text (the OCR will return wrong text in place of blacklisted characters otherwise), you can blacklist those characters by using the following config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = r'-c tessedit_char_blacklist=abcdefghijklmnopqrstuvwxyz --psm 6'\n",
    "pytesseract.image_to_string(img, config=custom_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output :\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitg0gkbp2j30jg013mxg.jpg)\n",
    "\n",
    "# Multiple languages text :\n",
    "\n",
    "To specify the language you need your OCR output in, use the `-l LANG` argument in the config where LANG is the 3 letter code for what language you want to use.\n",
    "\n",
    "You can work with multiple languages by changing the LANG parameter as such :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_config = r'-l grc+tha+eng --psm 6'\n",
    "pytesseract.image_to_string(img, config=custom_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : The language specified first to the -l parameter is the primary language.\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitg15whl7j30jg0bbgnk.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you will get the following output :\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/007S8ZIlly1gitg25fms6j30jg03lt9m.jpg)\n",
    "\n",
    "Unfortunately tesseract does not have a feature to detect language of the text in an image automatically. An alternative solution is provided by another python module called `langdetect` which can be installed via pip for more information check this [link](https://pypi.org/project/langdetect/).\n",
    "\n",
    "This module again, does not detect the language of text using an image but needs string input to detect the language from. The best way to do this is by first using tesseract to get OCR text in whatever languages you might feel are in there, using `langdetect` to find what languages are included in the OCR text and then run OCR again with the languages found.\n",
    "\n",
    "Say we have a text we thought was in english and portugese.\n",
    "\n",
    "**NB:** Tesseract performs badly when, in an image with multiple languages, the languages specified in the config are wrong or aren’t mentioned at all. This can mislead the langdetect module quite a bit as well.\n",
    "\n",
    "# Tesseract limitations :\n",
    "\n",
    "Tesseract OCR is quite powerful but does have some limitations.\n",
    "\n",
    "* The OCR is not as accurate as some available commercial solutions .\n",
    "* Doesn’t do well with images affected by artifacts including partial occlusion, distorted perspective, and complex background.\n",
    "* It is not capable of recognizing handwriting.\n",
    "* It may find gibberish and report this as OCR output.\n",
    "* If a document contains languages outside of those given in the -l LANG arguments, results may be poor.\n",
    "* It is not always good at analyzing the natural reading order of documents. For example, it may fail to recognize that a document contains two columns, and may try to join text across columns.\n",
    "* Poor quality scans may produce poor quality OCR.\n",
    "* It does not expose information about what font family text belongs to.\n",
    "\n",
    "# Conclusion :\n",
    "\n",
    "Tesseract is perfect for scanning clean documents and comes with pretty high accuracy and font variability since its training was comprehensive.\n",
    "\n",
    "The latest release of Tesseract 4.0 supports deep learning based OCR that is significantly more accurate. The OCR engine itself is built on a Long Short-Term Memory (LSTM) network, which is a particular type of Recurrent Neural Network (RNN).\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
